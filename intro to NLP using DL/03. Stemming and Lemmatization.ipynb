{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1f9bb09",
   "metadata": {},
   "source": [
    "# Stemming(어간 추출) and Lemmatization(표제어 추출)\n",
    "- 지금까지의 자연어 처리의 단계는 단어의 개수를 줄이는데 초점이 맞춰짐\n",
    "    - 결국 표본공간의 크기를 줄이는 것이 중요\n",
    "- 이 단계는 표면적 의미가 아닌 실제 의미가 같으면 하나의 단어로 일반화하는 과정\n",
    "- 표제어 추출 과정은 어간 추출보다 사용된 형태를 잘 보존하는 양상을 보임\n",
    "    - 표제어 추출은 POS 태그를 보존함\n",
    "    - 어간 추출은 사전에 존재하지 않는 형태로 출력하는 경우도 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c2d0152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']\n"
     ]
    }
   ],
   "source": [
    "# Lemma = 표제어, 기본형(뿌리) 단어 ex) am, are is -> be\n",
    "# Stem = 어간, 실제 의미를 담고 있는 핵심 부분\n",
    "# affix = 접사, 추가적 의미를 부여하는 부분\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words =  ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
    "print([lemmatizer.lemmatize(w) for w in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81ebcc46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'die'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'dy'같은 형태가 출력되는 것은, 표제어 추출 과정에서 품사정보가 필요하기 때문\n",
    "lemmatizer.lemmatize('dies', 'v')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375b857b",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76982139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'same', 'research', 'also', 'revealed', 'the', 'network', 'of', '257', 'World', 'Heritage', 'forests', 'around', 'the', 'world', 'collectively', 'removed', '190', 'million', 'tonnes', 'of', 'carbon', 'from', 'the', 'atmosphere', 'every', 'year', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "porter_stemmer = PorterStemmer()\n",
    "text = \"The same research also revealed the network of 257 World Heritage forests around the world collectively removed 190 million tonnes of carbon from the atmosphere every year.\"\n",
    "words = word_tokenize(text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c43c7e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'same', 'research', 'also', 'reveal', 'the', 'network', 'of', '257', 'world', 'heritag', 'forest', 'around', 'the', 'world', 'collect', 'remov', '190', 'million', 'tonn', 'of', 'carbon', 'from', 'the', 'atmospher', 'everi', 'year', '.']\n"
     ]
    }
   ],
   "source": [
    "# 어간 추출은 단순 규칙 기반이기 때문에 remov, heritag 같은 단어가 등장함\n",
    "print([porter_stemmer.stem(w) for w in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b6251d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'sam', 'research', 'also', 'rev', 'the', 'network', 'of', '257', 'world', 'herit', 'forest', 'around', 'the', 'world', 'collect', 'remov', '190', 'mil', 'ton', 'of', 'carbon', 'from', 'the', 'atmosph', 'every', 'year', '.']\n"
     ]
    }
   ],
   "source": [
    "# 상기했듯이, 단순 규칙기반이기 때문에 알고리즘별 규칙 차이가 존재함\n",
    "from nltk.stem import LancasterStemmer\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "print([lancaster_stemmer.stem(w) for w in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "664b32f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['polit', 'allow', 'electric', 'organ']\n"
     ]
    }
   ],
   "source": [
    "words = ['political', 'allowance', 'electricical', 'organization']\n",
    "print([stemmer.stem(w) for w in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823e1b30",
   "metadata": {},
   "source": [
    "- 'organization'과 'organ'은 전혀 관계없는 단어임에도 단순 규칙에 따라 어간 추출을 진행하기 때문에 유의해야 한다"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
